---
phase: 12-elevenlabs-tts-upgrade
plan: 02
type: execute
wave: 2
depends_on: ["12-01"]
files_modified:
  - app/api/library_routes.py
  - app/services/tts/base.py
  - supabase/migrations/009_add_tts_timestamps_to_clips.sql
autonomous: true

must_haves:
  truths:
    - "Render endpoint accepts elevenlabs_model parameter to select model per render"
    - "Render task uses selected model when calling ElevenLabs TTS"
    - "TTS timestamp data is persisted in Supabase for downstream subtitle generation"
    - "Timestamp data includes character-level timing aligned with generated audio"
  artifacts:
    - path: "app/api/library_routes.py"
      provides: "Render endpoint with model selection and timestamp persistence"
      contains: "elevenlabs_model"
    - path: "app/services/tts/base.py"
      provides: "Extended TTSResult with optional timestamps field"
      contains: "timestamps"
    - path: "supabase/migrations/009_add_tts_timestamps_to_clips.sql"
      provides: "Database column for storing TTS timestamp data"
      contains: "tts_timestamps"
  key_links:
    - from: "app/api/library_routes.py"
      to: "app/services/tts/elevenlabs.py"
      via: "generate_audio_with_timestamps call in _render_final_clip_task"
      pattern: "generate_audio_with_timestamps"
    - from: "app/api/library_routes.py"
      to: "Supabase editai_clip_content"
      via: "Stores tts_timestamps JSON after TTS generation"
      pattern: "tts_timestamps"
---

<objective>
Wire the new `generate_audio_with_timestamps()` method into the render pipeline, add a
`elevenlabs_model` Form parameter to let users select the model per render, and persist
the character-level timestamp data in Supabase for downstream subtitle generation (Phase 13).

Purpose: Completes the backend integration for TTS-03 (timestamp persistence) and TTS-04
(model selection per render). Makes timestamp data available for Phase 13's TTS-based subtitles.

Output:
- Updated render endpoint with `elevenlabs_model` parameter
- Render task calls `generate_audio_with_timestamps()` and stores alignment data
- Database migration for `tts_timestamps` JSONB column
- Extended TTSResult dataclass with optional timestamps
</objective>

<execution_context>
@/home/ukfdb/.claude/get-shit-done/workflows/execute-plan.md
@/home/ukfdb/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-elevenlabs-tts-upgrade/12-01-SUMMARY.md

@app/api/library_routes.py
@app/services/tts/base.py
@app/services/tts/elevenlabs.py
@app/services/elevenlabs_tts.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add model selection to render endpoint and use timestamps in render task</name>
  <files>app/api/library_routes.py, app/services/tts/base.py, supabase/migrations/009_add_tts_timestamps_to_clips.sql</files>
  <action>
  **1. Extend TTSResult (app/services/tts/base.py):**
  Add optional `timestamps` field to the `TTSResult` dataclass:
  ```python
  @dataclass
  class TTSResult:
      """Result of TTS audio generation."""
      audio_path: Path
      duration_seconds: float
      provider: str
      voice_id: str
      cost: float
      timestamps: Optional[dict] = None  # Character-level timing data from /with-timestamps
  ```
  Add `from typing import ... Optional` import if not present.

  **2. Create migration (supabase/migrations/009_add_tts_timestamps_to_clips.sql):**
  ```sql
  -- Migration 009: Add tts_timestamps JSONB column to editai_clip_content
  -- Stores character-level timing data from ElevenLabs /with-timestamps endpoint
  -- Used by Phase 13 for TTS-based subtitle generation

  ALTER TABLE editai_clip_content
  ADD COLUMN IF NOT EXISTS tts_timestamps JSONB DEFAULT NULL;

  -- Also add elevenlabs_model to track which model was used for this clip's TTS
  ALTER TABLE editai_clip_content
  ADD COLUMN IF NOT EXISTS tts_model TEXT DEFAULT NULL;

  COMMENT ON COLUMN editai_clip_content.tts_timestamps IS 'ElevenLabs character-level timing data: {characters, character_start_times_seconds, character_end_times_seconds}';
  COMMENT ON COLUMN editai_clip_content.tts_model IS 'ElevenLabs model used for TTS generation (e.g. eleven_flash_v2_5)';
  ```

  **3. Update render endpoint (app/api/library_routes.py):**

  In `render_final_clip()` function (the `@router.post("/clips/{clip_id}/render")` handler):
  - Add Form parameter: `elevenlabs_model: str = Form(default="eleven_flash_v2_5")`
    Place it after the existing Form params, before the `profile` dependency.
  - Pass `elevenlabs_model` to the background task call:
    Add `elevenlabs_model=elevenlabs_model` to the `background_tasks.add_task()` kwargs.

  In `_render_final_clip_task()` function signature:
  - Add parameter: `elevenlabs_model: str = "eleven_flash_v2_5"`

  In `_render_final_clip_task()` body, replace the TTS generation block (lines ~1789-1807):

  The current code does:
  ```python
  if content_data and content_data.get("tts_text"):
      tts = get_elevenlabs_tts()
      audio_path = temp_dir / f"tts_{clip_id}.mp3"
      audio_path, silence_stats = tts.generate_audio_trimmed(...)
  ```

  Replace with:
  ```python
  if content_data and content_data.get("tts_text"):
      # Use new TTS service with timestamps support
      from app.services.tts.elevenlabs import ElevenLabsTTSService
      from app.config import get_settings
      settings_tts = get_settings()

      tts_service = ElevenLabsTTSService(
          output_dir=temp_dir,
          model_id=elevenlabs_model  # User-selected model
      )

      audio_path = temp_dir / f"tts_{clip_id}.mp3"
      tts_timestamps = None

      try:
          # Generate with timestamps for downstream subtitle sync
          import asyncio
          tts_result, tts_timestamps = await tts_service.generate_audio_with_timestamps(
              text=content_data["tts_text"],
              voice_id=tts_service._voice_id,
              output_path=audio_path,
              model_id=elevenlabs_model
          )
          audio_path = tts_result.audio_path
          logger.info(f"TTS with timestamps generated for clip {clip_id}: {tts_result.duration_seconds:.1f}s, model={elevenlabs_model}")
      except Exception as e:
          # Fallback to legacy TTS without timestamps
          logger.warning(f"Timestamps generation failed, falling back to standard TTS: {e}")
          from app.services.elevenlabs_tts import get_elevenlabs_tts
          tts = get_elevenlabs_tts()
          audio_path, _ = tts.generate_audio_trimmed(
              text=content_data["tts_text"],
              output_path=audio_path,
              remove_silence=True,
              min_silence_duration=0.25,
              silence_padding=0.06
          )

      # Apply silence removal to timestamped audio
      if tts_timestamps:
          try:
              from app.services.silence_remover import SilenceRemover
              remover = SilenceRemover(min_silence_duration=0.25, padding=0.06)
              trimmed_path = temp_dir / f"tts_trimmed_{clip_id}.mp3"
              silence_result = remover.remove_silence(audio_path, trimmed_path)
              audio_path = trimmed_path
              logger.info(f"Silence removal: {silence_result.original_duration:.1f}s -> {silence_result.new_duration:.1f}s")
          except Exception as e:
              logger.warning(f"Silence removal failed, using raw audio: {e}")

      audio_duration = _get_audio_duration(audio_path)

      # Persist timestamps and model to Supabase for Phase 13 subtitle generation
      if tts_timestamps:
          try:
              supabase.table("editai_clip_content").update({
                  "tts_timestamps": tts_timestamps,
                  "tts_model": elevenlabs_model
              }).eq("clip_id", clip_id).execute()
              logger.info(f"TTS timestamps persisted for clip {clip_id}")
          except Exception as e:
              logger.warning(f"Failed to persist TTS timestamps: {e}")

      if silence_stats := (None if tts_timestamps else None):
          pass  # silence_stats handled above
      logger.info(f"TTS generated for clip {clip_id}: {audio_duration:.1f}s")
  ```

  IMPORTANT NOTES:
  - The `_render_final_clip_task` is already `async`, so `await` works directly.
  - Keep the existing sync logic (video trim/extend) below the TTS section unchanged.
  - The legacy `from app.services.elevenlabs_tts import get_elevenlabs_tts` stays as fallback.
  - The `silence_stats` variable referenced later in the function should still work. Check the
    downstream code that references it and ensure it doesn't break. If `silence_stats` is used
    after the TTS block, initialize it to `None` at the start of the block and set it appropriately.
  </action>
  <verify>
  Run: `cd "/mnt/c/OBSID SRL/n8n/edit_factory" && python -c "
from app.api.library_routes import render_final_clip
import inspect
sig = inspect.signature(render_final_clip)
params = list(sig.parameters.keys())
assert 'elevenlabs_model' in params, f'Missing elevenlabs_model param. Found: {params}'
print(f'Render endpoint OK: elevenlabs_model param present. All params: {params}')
"` -- should show elevenlabs_model in params.

  Check migration file exists: `ls supabase/migrations/009_add_tts_timestamps_to_clips.sql`

  Grep for `tts_timestamps` in `app/api/library_routes.py` to confirm persistence code.
  Grep for `generate_audio_with_timestamps` in `app/api/library_routes.py` to confirm new method is called.
  </verify>
  <done>
  - Render endpoint accepts `elevenlabs_model` Form parameter (default: `eleven_flash_v2_5`)
  - Render task calls `generate_audio_with_timestamps()` with user-selected model
  - Timestamp alignment data persisted to `editai_clip_content.tts_timestamps` JSONB column
  - Model name persisted to `editai_clip_content.tts_model` TEXT column
  - Graceful fallback to legacy TTS if timestamps endpoint fails
  - Migration 009 created for new database columns
  - TTSResult dataclass extended with optional `timestamps` field
  </done>
</task>

</tasks>

<verification>
1. `render_final_clip` endpoint has `elevenlabs_model` Form parameter
2. `_render_final_clip_task` receives and uses `elevenlabs_model` parameter
3. Render task calls `generate_audio_with_timestamps()` and falls back to legacy on failure
4. Timestamp data is stored in Supabase `editai_clip_content.tts_timestamps`
5. Migration 009 SQL file exists with `tts_timestamps` JSONB and `tts_model` TEXT columns
6. TTSResult has `timestamps` optional field
7. Python import and syntax check passes
</verification>

<success_criteria>
- TTS-03 FULLY SATISFIED: Timestamps retrieved AND persisted for downstream subtitle generation
- TTS-04 BACKEND SATISFIED: Model selection available per render via `elevenlabs_model` parameter
  (Frontend model selector in Plan 03)
</success_criteria>

<output>
After completion, create `.planning/phases/12-elevenlabs-tts-upgrade/12-02-SUMMARY.md`
</output>
