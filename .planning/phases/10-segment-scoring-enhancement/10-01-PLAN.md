---
phase: 10-segment-scoring-enhancement
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/services/video_processor.py
  - app/models.py
autonomous: true

must_haves:
  truths:
    - "System calculates blur score using Laplacian variance for each segment"
    - "Blurry segments (Laplacian variance < 100) are rejected before scoring"
    - "Combined scoring uses 5 factors: motion 40%, variance 20%, blur 20%, contrast 15%, brightness 5%"
    - "No single scoring factor dominates segment selection"
    - "Scoring performance overhead is < 5% vs current (blur/contrast computed on 3 sampled frames)"
  artifacts:
    - path: "app/services/video_processor.py"
      provides: "Enhanced VideoSegment dataclass with blur_score and contrast_score fields, blur/contrast calculation methods, updated scoring weights"
      contains: "_calculate_blur_score"
    - path: "app/models.py"
      provides: "Pydantic VideoSegment model with optional blur_score and contrast_score"
      contains: "blur_score"
  key_links:
    - from: "VideoAnalyzer._calculate_motion_for_interval"
      to: "VideoAnalyzer._calculate_blur_score + _calculate_contrast_score"
      via: "calls blur/contrast on 3 sampled frames within the interval loop"
      pattern: "_calculate_blur_score|_calculate_contrast_score"
    - from: "VideoAnalyzer.analyze_full_video"
      to: "VideoSegment constructor"
      via: "unpacks 4 return values from _calculate_motion_for_interval"
      pattern: "motion_score, variance_score, blur_score, contrast_score"
    - from: "VideoSegment.to_dict"
      to: "app/models.py VideoSegment"
      via: "dict includes blur_score and contrast_score for API serialization"
      pattern: "blur_score.*contrast_score"
---

<objective>
Add blur detection (Laplacian variance) and contrast analysis (standard deviation) to the video segment scoring algorithm, rebalancing weights from 3-factor (60/30/10) to 5-factor (40/20/20/15/5) scoring.

Purpose: Currently, segment selection only considers motion, variance, and brightness. This means blurry or low-contrast segments can rank highly if they have good motion. Adding blur/contrast scoring produces visibly sharper, more aesthetically pleasing clip selections.

Output: Enhanced `video_processor.py` with blur detection, contrast measurement, updated scoring weights, and blur rejection threshold.
</objective>

<execution_context>
@/home/ukfdb/.claude/get-shit-done/workflows/execute-plan.md
@/home/ukfdb/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-segment-scoring-enhancement/10-RESEARCH.md
@app/services/video_processor.py
@app/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add blur/contrast scoring to VideoAnalyzer and VideoSegment</name>
  <files>app/services/video_processor.py</files>
  <action>
Modify `app/services/video_processor.py` with these specific changes:

**1. Extend VideoSegment dataclass (line ~62, before visual_hashes):**
Add two new fields with defaults so existing constructors don't break:
```python
blur_score: float = 1.0       # Sharpness: 1.0 = sharp, 0.0 = blurry (Laplacian variance normalized)
contrast_score: float = 0.5   # Contrast level: 0-1 (std dev normalized)
```

**2. Update combined_score property (lines 69-77):**
Replace the 3-factor scoring with 5-factor:
```python
@property
def combined_score(self) -> float:
    """Enhanced scoring: motion, variance, blur, contrast, brightness."""
    return (
        self.motion_score * 0.40 +
        self.variance_score * 0.20 +
        self.blur_score * 0.20 +
        self.contrast_score * 0.15 +
        (1 - abs(self.avg_brightness - 0.5)) * 0.05
    )
```

**3. Update to_dict() (line ~96-104):**
Add blur_score and contrast_score to the returned dict:
```python
"blur_score": round(self.blur_score, 4),
"contrast_score": round(self.contrast_score, 4)
```

**4. Add two new methods to VideoAnalyzer class (after _read_frame_at, before _calculate_motion_for_interval):**

`_calculate_blur_score(self, frame: np.ndarray) -> float`:
- Convert to grayscale if BGR (check frame.shape length)
- Compute `cv2.Laplacian(gray, cv2.CV_64F).var()`
- Normalize: `min(laplacian_var / 500.0, 1.0)`
- Return normalized score (0.0 = blurry, 1.0 = sharp)
- IMPORTANT: Do NOT apply GaussianBlur before Laplacian -- pass the raw grayscale frame, not the blurred one used for motion detection

`_calculate_contrast_score(self, frame: np.ndarray) -> float`:
- Convert to grayscale if BGR
- Compute `np.std(gray)`
- Normalize: `min(contrast / 80.0, 1.0)`
- Return normalized score

**5. Update _calculate_motion_for_interval (lines 172-226):**
- Change return type annotation from `Tuple[float, float]` to `Tuple[float, float, float, float]`
- Change docstring to mention blur and contrast
- Change early return from `return 0.0, 0.0` to `return 0.0, 0.0, 1.0, 0.5`
- Add `blur_scores = []` and `contrast_scores = []` lists at initialization (alongside motion_scores, frames_gray)
- Inside the frame loop, AFTER `gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)` but BEFORE the GaussianBlur (so we use the unblurred gray for quality metrics), add sampling logic:
  ```python
  # Blur/contrast on first 3 frames only (performance: <5% overhead)
  if len(blur_scores) < 3:
      blur_scores.append(self._calculate_blur_score(gray))
      contrast_scores.append(self._calculate_contrast_score(gray))
  ```
- After computing motion_score and variance_score, add:
  ```python
  blur_score = np.mean(blur_scores) if blur_scores else 1.0
  contrast_score = np.mean(contrast_scores) if contrast_scores else 0.5
  ```
- Change return to: `return motion_score, variance_score, blur_score, contrast_score`

**6. Update analyze_full_video (lines 256-315):**
- Line 264: Change unpacking from `motion_score, variance_score = ...` to `motion_score, variance_score, blur_score, contrast_score = ...`
- Add blur rejection threshold after the existing thresholds (after line 284):
  ```python
  MIN_BLUR_THRESHOLD = 0.2  # Reject very blurry segments (Laplacian variance < 100)
  ```
- Add `is_too_blurry = blur_score < MIN_BLUR_THRESHOLD` after `is_too_static`
- Add blur rejection in the if/elif chain (after is_too_static check):
  ```python
  elif is_too_blurry:
      logger.debug(f"Skipped BLURRY segment: {start_time:.1f}s - {end_time:.1f}s (blur: {blur_score:.3f})")
  ```
- In the VideoSegment constructor (line 294-301), add the new fields:
  ```python
  blur_score=blur_score,
  contrast_score=contrast_score,
  ```

**7. Update _gemini_to_video_segments (line 1122):**
The VideoSegment constructor here uses defaults, so blur_score=1.0 and contrast_score=0.5 defaults apply automatically. No change needed since fields have defaults.

**8. Similarly for process_video Gemini fallback (line 1482) and process_video_smart (line 1199):**
These also construct VideoSegment and will use defaults. No change needed.

IMPORTANT: The blur/contrast computations reuse the grayscale frame already being computed for motion detection. The only added operations are cv2.Laplacian and np.std on 3 frames per segment -- minimal overhead.
  </action>
  <verify>
Run `python -c "from app.services.video_processor import VideoSegment, VideoAnalyzer; s = VideoSegment(start_time=0, end_time=3, motion_score=0.5, variance_score=0.3, avg_brightness=0.5, blur_score=0.8, contrast_score=0.6); print(f'combined={s.combined_score:.4f}'); print(s.to_dict()); s2 = VideoSegment(start_time=0, end_time=3, motion_score=0.5, variance_score=0.3, avg_brightness=0.5); print(f'defaults: blur={s2.blur_score}, contrast={s2.contrast_score}')"` from project root.

Expected: combined score uses all 5 factors (0.5*0.4 + 0.3*0.2 + 0.8*0.2 + 0.6*0.15 + 0.5*0.05 = 0.200 + 0.060 + 0.160 + 0.090 + 0.025 = 0.535). to_dict includes blur_score and contrast_score. Defaults work (blur=1.0, contrast=0.5).
  </verify>
  <done>
VideoSegment has blur_score and contrast_score fields with backward-compatible defaults. Combined score uses 40/20/20/15/5 weights. _calculate_motion_for_interval returns 4 values. analyze_full_video rejects segments with blur_score < 0.2. Blur/contrast computed on 3 sampled frames per segment for minimal overhead.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update Pydantic model for API compatibility</name>
  <files>app/models.py</files>
  <action>
Update the Pydantic `VideoSegment` model in `app/models.py` (line 17-23) to include the new scoring fields as optional (backward compatible with existing API consumers):

```python
class VideoSegment(BaseModel):
    start: float
    end: float
    duration: float
    motion_score: float
    variance_score: Optional[float] = None
    combined_score: float
    blur_score: Optional[float] = None
    contrast_score: Optional[float] = None
```

These are Optional because:
1. Gemini-analyzed segments don't compute blur/contrast (they use defaults)
2. Existing API consumers shouldn't break if they don't expect these fields
3. The fields will be populated from VideoSegment.to_dict() which now includes them
  </action>
  <verify>
Run `python -c "from app.models import VideoSegment; s = VideoSegment(start=0, end=3, duration=3, motion_score=0.5, combined_score=0.4); print(s.model_dump()); s2 = VideoSegment(start=0, end=3, duration=3, motion_score=0.5, combined_score=0.4, blur_score=0.8, contrast_score=0.6); print(s2.model_dump())"` from project root.

Expected: First segment has blur_score=None, contrast_score=None. Second has blur_score=0.8, contrast_score=0.6. No validation errors.
  </verify>
  <done>
Pydantic VideoSegment model accepts optional blur_score and contrast_score. API responses include new fields when present. Backward compatible -- existing consumers that don't parse these fields continue to work.
  </done>
</task>

</tasks>

<verification>
1. Import check: `python -c "from app.services.video_processor import VideoSegment, VideoAnalyzer; print('OK')"` succeeds
2. Scoring math: VideoSegment with known values produces expected combined_score using 5-factor formula
3. Backward compatibility: VideoSegment constructed without blur_score/contrast_score uses defaults (1.0, 0.5)
4. API model: Pydantic VideoSegment accepts optional blur_score and contrast_score
5. to_dict output: Includes blur_score and contrast_score keys
6. No new dependencies: Only cv2 and numpy used (already in requirements.txt)
</verification>

<success_criteria>
1. VideoSegment dataclass has blur_score (default 1.0) and contrast_score (default 0.5) fields
2. combined_score property uses weights: motion 40%, variance 20%, blur 20%, contrast 15%, brightness 5%
3. _calculate_blur_score uses cv2.Laplacian(gray, cv2.CV_64F).var() normalized by /500.0
4. _calculate_contrast_score uses np.std(gray) normalized by /80.0
5. _calculate_motion_for_interval returns 4 values (motion, variance, blur, contrast)
6. analyze_full_video rejects segments with blur_score < 0.2
7. Blur/contrast computed on max 3 frames per segment (performance constraint)
8. All existing VideoSegment constructors continue to work (backward-compatible defaults)
9. Pydantic VideoSegment model includes optional blur_score and contrast_score
</success_criteria>

<output>
After completion, create `.planning/phases/10-segment-scoring-enhancement/10-01-SUMMARY.md`
</output>
