---
phase: 29-testing-and-observability
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - app/logging_config.py
  - app/main.py
  - app/cleanup.py
  - requirements.txt
autonomous: true
requirements:
  - TEST-03
  - TEST-04

must_haves:
  truths:
    - "Backend log output is valid JSON parseable by log aggregators"
    - "A CLI command removes temp files and failed jobs older than the retention window"
  artifacts:
    - path: "app/logging_config.py"
      provides: "Structured JSON logging configuration"
      contains: "json"
      min_lines: 20
    - path: "app/main.py"
      provides: "Imports and calls structured logging setup at startup"
      contains: "logging_config"
    - path: "app/cleanup.py"
      provides: "Data retention CLI script"
      contains: "cleanup"
      min_lines: 30
    - path: "requirements.txt"
      provides: "python-json-logger dependency"
      contains: "python-json-logger"
  key_links:
    - from: "app/main.py"
      to: "app/logging_config.py"
      via: "import and call at module level"
      pattern: "from app.logging_config import|setup_logging"
    - from: "app/cleanup.py"
      to: "app/services/job_storage.py"
      via: "calls cleanup_old_jobs"
      pattern: "cleanup_old_jobs|JobStorage"
---

<objective>
Replace plain text logging with structured JSON logging and create a data retention cleanup command.

Purpose: Enable log aggregation/parsing and prevent unbounded growth of temp files and stale job records.
Output: JSON-formatted log output from all backend modules, a runnable cleanup script for temp/job retention.
</objective>

<execution_context>
@/home/ukfdb/.claude/get-shit-done/workflows/execute-plan.md
@/home/ukfdb/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@app/main.py
@app/services/job_storage.py
@app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement structured JSON logging</name>
  <files>app/logging_config.py, app/main.py, requirements.txt</files>
  <action>
    1. Add `python-json-logger>=2.0.0` to `requirements.txt`.

    2. Create `app/logging_config.py`:
       ```python
       import logging
       import sys
       from pythonjsonlogger import jsonlogger

       def setup_logging(level=logging.INFO):
           """Configure structured JSON logging for all loggers."""
           handler = logging.StreamHandler(sys.stdout)
           formatter = jsonlogger.JsonFormatter(
               fmt="%(asctime)s %(name)s %(levelname)s %(message)s",
               rename_fields={"asctime": "timestamp", "levelname": "level", "name": "logger"},
               datefmt="%Y-%m-%dT%H:%M:%S"
           )
           handler.setFormatter(formatter)

           root = logging.getLogger()
           root.handlers.clear()
           root.addHandler(handler)
           root.setLevel(level)

           # Quieten noisy third-party loggers
           for noisy in ["httpcore", "httpx", "urllib3", "multipart"]:
               logging.getLogger(noisy).setLevel(logging.WARNING)
       ```

    3. Update `app/main.py`:
       - Replace the existing `logging.basicConfig(...)` block (lines 41-44) with:
         ```python
         from app.logging_config import setup_logging
         setup_logging()
         ```
       - Remove the old `logging.basicConfig` call entirely. Keep the `logger = logging.getLogger(__name__)` line.
       - Do NOT change anything else in main.py.
  </action>
  <verify>
    Start the backend with `python -c "from app.logging_config import setup_logging; setup_logging(); import logging; logging.getLogger('test').info('hello')"` — output should be a single JSON line with "timestamp", "level", "logger", "message" keys.
  </verify>
  <done>All backend log output is JSON-formatted with timestamp, level, logger, and message fields.</done>
</task>

<task type="auto">
  <name>Task 2: Create data retention cleanup command</name>
  <files>app/cleanup.py</files>
  <action>
    Create `app/cleanup.py` as a standalone CLI script (runnable via `python -m app.cleanup`):

    1. **Temp file cleanup**: Walk `temp/` directory tree. Delete files older than `--days` (default 7). Delete empty directories after file removal. Count and report deleted files.

    2. **Failed job cleanup**: Use JobStorage's existing `cleanup_old_jobs(days)` method for Supabase-backed cleanup. For in-memory mode, iterate `_memory_store` and remove jobs with status "failed" and created_at older than cutoff.

    3. **Output directory cleanup**: Walk `output/` for files older than `--days`. Only delete files (not the directory itself). Count and report.

    4. **CLI interface** using argparse:
       - `--days N` (default 7): retention window in days
       - `--dry-run`: print what would be deleted without deleting
       - `--temp-only`: only clean temp files
       - `--jobs-only`: only clean old jobs

    5. Add `if __name__ == "__main__":` block so it runs with `python -m app.cleanup --days 7`.

    6. Use structured logging (import setup_logging from logging_config) so cleanup output is also JSON.

    Script should handle missing directories gracefully (temp/ or output/ may not exist).
  </action>
  <verify>
    Run `cd "/mnt/c/OBSID SRL/n8n/edit_factory" && python -m app.cleanup --dry-run --days 7` — should complete without errors and report "0 files" or list files that would be deleted.
  </verify>
  <done>Running `python -m app.cleanup --days 7` removes temp files and failed jobs older than 7 days; `--dry-run` previews without deleting.</done>
</task>

</tasks>

<verification>
1. Start backend — log output is valid JSON (pipe through `python -m json.tool` to verify)
2. `python -m app.cleanup --dry-run` runs without errors
3. `python -m app.cleanup --days 0` in a test scenario removes old temp files
</verification>

<success_criteria>
- Backend log output is valid JSON parseable by log aggregators (TEST-03)
- Data retention command removes temp files and failed jobs older than retention window (TEST-04)
</success_criteria>

<output>
After completion, create `.planning/phases/29-testing-and-observability/29-02-SUMMARY.md`
</output>
