---
phase: 29-testing-and-observability
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - tests/conftest.py
  - tests/test_job_storage.py
  - tests/test_cost_tracker.py
  - tests/test_srt_validator.py
autonomous: true
requirements:
  - TEST-01
  - TEST-02

must_haves:
  truths:
    - "Running `pytest` from project root discovers and runs tests without errors"
    - "JobStorage in-memory fallback CRUD operations pass all tests"
    - "CostTracker calculates ElevenLabs and Gemini costs correctly"
    - "SRTValidator validates correct SRT, rejects malformed SRT, and sanitizes HTML/XSS"
  artifacts:
    - path: "pyproject.toml"
      provides: "pytest configuration (testpaths, pythonpath)"
      contains: "[tool.pytest.ini_options]"
    - path: "tests/conftest.py"
      provides: "Shared fixtures (tmp_path, mock settings, memory-only JobStorage, CostTracker)"
      min_lines: 20
    - path: "tests/test_job_storage.py"
      provides: "Unit tests for JobStorage CRUD and in-memory fallback"
      min_lines: 40
    - path: "tests/test_cost_tracker.py"
      provides: "Unit tests for CostTracker cost calculation and local JSON log"
      min_lines: 40
    - path: "tests/test_srt_validator.py"
      provides: "Unit tests for SRTValidator and sanitize_srt_text"
      min_lines: 40
  key_links:
    - from: "pyproject.toml"
      to: "tests/"
      via: "testpaths config"
      pattern: "testpaths.*tests"
    - from: "tests/conftest.py"
      to: "app/services/job_storage.py"
      via: "fixture creates JobStorage with _supabase=None"
      pattern: "JobStorage|_supabase"
    - from: "tests/test_cost_tracker.py"
      to: "app/services/cost_tracker.py"
      via: "CostTracker instantiated with tmp log_dir"
      pattern: "CostTracker|ELEVENLABS_COST_PER_CHAR"
---

<objective>
Set up pytest infrastructure and write unit tests for the three critical backend services: JobStorage, CostTracker, and SRTValidator.

Purpose: Establish a test harness so regressions in core services are caught before deployment.
Output: pyproject.toml with pytest config, conftest.py with shared fixtures, 3 test files covering CRUD/calculation/validation logic.
</objective>

<execution_context>
@/home/ukfdb/.claude/get-shit-done/workflows/execute-plan.md
@/home/ukfdb/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@app/services/job_storage.py
@app/services/cost_tracker.py
@app/services/srt_validator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create pytest configuration and shared fixtures</name>
  <files>pyproject.toml, tests/__init__.py, tests/conftest.py</files>
  <action>
    1. Create `pyproject.toml` at project root with pytest configuration:
       ```toml
       [tool.pytest.ini_options]
       testpaths = ["tests"]
       pythonpath = ["."]
       ```
       This ensures `pytest` discovers tests and `app.*` imports work.

    2. Create empty `tests/__init__.py`.

    3. Create `tests/conftest.py` with shared fixtures:
       - `mock_settings` fixture: patches `app.config.get_settings` to return a Settings-like object with `supabase_url=""`, `supabase_key=""`, `logs_dir` pointing to `tmp_path / "logs"`. This ensures no Supabase connection is attempted.
       - `memory_job_storage` fixture: creates a JobStorage instance with `_supabase = None` (force in-memory mode). Directly instantiate and set `_supabase = None` after init (the init will fail gracefully without env vars).
       - `cost_tracker` fixture: creates a CostTracker instance with `log_dir=tmp_path / "logs"` and `_supabase = None`.

    Use `unittest.mock.patch` for `app.config.get_settings` in fixtures that need it. Use `autouse=False` so tests opt-in explicitly.
  </action>
  <verify>Run `cd "/mnt/c/OBSID SRL/n8n/edit_factory" && python -m pytest tests/ --collect-only` — should discover test files without import errors.</verify>
  <done>pyproject.toml exists with pytest config, tests/conftest.py has 3 fixtures, `pytest --collect-only` succeeds.</done>
</task>

<task type="auto">
  <name>Task 2: Write unit tests for JobStorage, CostTracker, and SRTValidator</name>
  <files>tests/test_job_storage.py, tests/test_cost_tracker.py, tests/test_srt_validator.py</files>
  <action>
    **tests/test_job_storage.py** — Test the in-memory fallback path (no Supabase):
    - `test_create_job`: create_job with valid job_data returns the job; job appears in _memory_store
    - `test_create_job_missing_id`: create_job without job_id raises ValueError
    - `test_get_job_exists`: get_job returns previously created job
    - `test_get_job_not_found`: get_job for nonexistent ID returns None
    - `test_update_job`: update_job merges fields and updates updated_at
    - `test_update_job_not_found`: update_job for nonexistent ID returns None
    - `test_list_jobs_all`: list_jobs returns all created jobs
    - `test_list_jobs_filter_status`: list_jobs with status filter returns only matching
    - `test_list_jobs_filter_profile`: list_jobs with profile_id filter returns only matching
    - `test_delete_job`: delete_job removes job, returns True
    - `test_delete_job_not_found`: delete_job for nonexistent returns False

    For each test, instantiate JobStorage directly, set `_supabase = None`, then test. Use `unittest.mock.patch("app.config.get_settings")` to prevent real settings loading.

    **tests/test_cost_tracker.py** — Test local JSON log path (no Supabase):
    - `test_log_elevenlabs_tts_cost`: log 1000 chars, verify cost_usd == round(1000 * 0.00022, 6)
    - `test_log_gemini_analysis_cost`: log 5 frames, verify cost includes frame cost + token estimate
    - `test_entries_persisted_to_file`: after logging, cost_log.json file exists and contains the entry
    - `test_get_summary_local`: after logging both types, get_summary returns correct totals
    - `test_check_quota_unlimited`: check_quota with quota=0 returns (False, 0.0, 0.0)
    - `test_check_quota_under`: log small cost, check_quota with high quota returns not exceeded
    - `test_check_quota_exceeded`: log enough cost to exceed quota, verify exceeded=True

    Use tmp_path for log_dir. Patch `app.config.get_settings` during CostTracker init.

    **tests/test_srt_validator.py** — Test validation and sanitization (no external deps):
    - `test_valid_srt`: validate_content with well-formed SRT returns (True, [])
    - `test_empty_srt`: validate_content with empty string returns (False, errors)
    - `test_invalid_timestamp`: validate_content with "99:99:99,999" returns errors
    - `test_end_before_start`: validate_content with end < start returns error
    - `test_missing_text`: validate_content with timestamp but no text returns error
    - `test_parse_entries`: parse_entries returns list of SRTEntry with correct fields
    - `test_fix_dot_timestamps`: fix_common_issues converts "00:00:01.500" to "00:00:01,500"
    - `test_sanitize_removes_script`: sanitize_srt_text strips `<script>alert(1)</script>`
    - `test_sanitize_removes_html_tags`: sanitize_srt_text strips `<b>bold</b>` to "bold"
    - `test_sanitize_preserves_arrow`: sanitize_srt_text does not strip "-->" in timestamps
    - `test_timestamp_to_seconds`: timestamp_to_seconds("01:30:45,500") == 5445.5

    SRTValidator and sanitize_srt_text have no external dependencies, so test directly without mocks.
  </action>
  <verify>Run `cd "/mnt/c/OBSID SRL/n8n/edit_factory" && python -m pytest tests/ -v` — all tests pass with 0 failures.</verify>
  <done>29+ unit tests pass covering JobStorage CRUD, CostTracker cost calculation, and SRTValidator validation/sanitization.</done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/ -v` — all tests pass
2. `python -m pytest tests/ --collect-only` — discovers all test files
3. No import errors or Supabase connection attempts during test run
</verification>

<success_criteria>
- pytest runs from project root without configuration errors (TEST-01)
- All unit tests for job_storage, cost_tracker, srt_validator pass (TEST-02)
</success_criteria>

<output>
After completion, create `.planning/phases/29-testing-and-observability/29-01-SUMMARY.md`
</output>
