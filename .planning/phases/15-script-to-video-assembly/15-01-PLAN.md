---
phase: 15-script-to-video-assembly
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/services/assembly_service.py
  - app/api/assembly_routes.py
  - app/main.py
autonomous: true
must_haves:
  truths:
    - "System matches SRT subtitle phrases against segment library keywords and returns ranked matches with confidence scores"
    - "System arranges matched segments into a timeline that covers the full TTS audio duration"
    - "Silence removal is applied to TTS audio before timeline calculation"
    - "API returns preview data showing which segments matched which script phrases"
    - "API triggers full render with matched segments, TTS audio, and subtitles using existing v3 quality settings"
  artifacts:
    - path: "app/services/assembly_service.py"
      provides: "Script-to-video assembly engine: TTS generation, SRT matching, timeline building, segment concatenation, render orchestration"
      min_lines: 200
    - path: "app/api/assembly_routes.py"
      provides: "Assembly API routes: preview matching, trigger assembly render"
      min_lines: 100
    - path: "app/main.py"
      provides: "Assembly router registration"
      contains: "assembly_router"
  key_links:
    - from: "app/api/assembly_routes.py"
      to: "app/services/assembly_service.py"
      via: "AssemblyService class instantiation"
      pattern: "AssemblyService"
    - from: "app/services/assembly_service.py"
      to: "app/services/tts/elevenlabs.py"
      via: "TTS generation with timestamps"
      pattern: "ElevenLabsTTSService"
    - from: "app/services/assembly_service.py"
      to: "app/services/tts_subtitle_generator.py"
      via: "SRT generation from timestamps"
      pattern: "generate_srt_from_timestamps"
    - from: "app/services/assembly_service.py"
      to: "app/services/silence_remover.py"
      via: "Audio silence removal"
      pattern: "SilenceRemover"
    - from: "app/main.py"
      to: "app/api/assembly_routes.py"
      via: "FastAPI router registration"
      pattern: "include_router.*assembly"
---

<objective>
Create the backend assembly service and API routes for the Script-to-Video Assembly pipeline.

Purpose: This is the core engine that takes a script, generates TTS audio with timestamps, matches SRT subtitle phrases against segment library keywords, builds a timeline of matched segments, and renders the final video. It bridges scripts (Phase 14), TTS with timestamps (Phase 12), auto-SRT (Phase 13), and the existing render pipeline.

Output: `app/services/assembly_service.py` (assembly engine), `app/api/assembly_routes.py` (API endpoints), updated `app/main.py` (router registration)
</objective>

<execution_context>
@/home/ukfdb/.claude/get-shit-done/workflows/execute-plan.md
@/home/ukfdb/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@app/services/tts/elevenlabs.py (ElevenLabsTTSService with generate_audio_with_timestamps)
@app/services/tts_subtitle_generator.py (generate_srt_from_timestamps)
@app/services/silence_remover.py (SilenceRemover)
@app/api/segments_routes.py (segment model, _parse_srt, keyword matching logic in match-srt endpoint)
@app/api/library_routes.py (existing _render_final_clip_task, _render_with_preset, _extend_video_with_segments patterns)
@app/api/script_routes.py (script generation endpoint for context)
@app/main.py (router registration pattern)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create assembly service with matching engine, timeline builder, and render orchestration</name>
  <files>app/services/assembly_service.py</files>
  <action>
Create `app/services/assembly_service.py` with an `AssemblyService` class that orchestrates the full script-to-video pipeline. The service must:

**1. TTS Generation (reuse existing)**
- Accept script text, generate TTS audio using `ElevenLabsTTSService.generate_audio_with_timestamps()`
- Apply silence removal using `SilenceRemover` (min_silence_duration=0.25, padding=0.06 — same params as _render_final_clip_task)
- Return audio path, duration, and timestamps dict

**2. SRT Generation (reuse existing)**
- Call `generate_srt_from_timestamps()` from tts_subtitle_generator to produce SRT content from timestamps
- Parse SRT into entries list using a local `_parse_srt()` (same logic as segments_routes.py)

**3. Keyword Matching Engine**
- Method `match_srt_to_segments(srt_entries, segments_data, min_confidence=0.3)` where segments_data is a list of dicts with {id, keywords, source_video_id, start_time, end_time, source_video_path, duration}
- For each SRT entry, find the best matching segment by checking if any segment keyword appears in the SRT text (case-insensitive)
- Confidence scoring: exact word match = 1.0, substring match = 0.7 (same as existing match-srt endpoint logic)
- If multiple segments match an SRT entry, pick the one with highest confidence, then longest duration (prefer more visual content)
- If no segment matches an SRT entry, mark it as "unmatched" (will use fallback segment or loop)
- Return list of `MatchResult` dataclasses: {srt_index, srt_text, srt_start, srt_end, segment_id, segment_keywords, confidence, matched_keyword}

**4. Timeline Builder**
- Method `build_timeline(match_results, audio_duration)` that arranges matched segments sequentially
- Each timeline entry specifies: source_video_path, start_time, end_time (within source), timeline_start (position in final video), timeline_duration
- For unmatched SRT entries: assign a fallback segment (first available, or loop previously used ones)
- Trim/extend segments to cover each SRT entry's time window (srt_start to srt_end)
- The total timeline duration must equal the audio duration
- Handle gap between last SRT entry end and audio end by extending the last segment or looping

**5. Video Assembly**
- Method `assemble_video(timeline, temp_dir)` that uses FFmpeg to:
  - Extract each segment clip from its source video using `ffmpeg -ss {start} -i {source} -t {duration}`
  - Create a concat list file
  - Concatenate all clips using `ffmpeg -f concat -safe 0`
  - Trim final concatenated video to exact audio duration
  - Return path to assembled video (no audio, just video track)

**6. Full Render Orchestration**
- Async method `assemble_and_render(script_text, profile_id, preset_data, subtitle_settings=None, elevenlabs_model="eleven_flash_v2_5", enable_denoise=False, denoise_strength=2.0, enable_sharpen=False, sharpen_amount=0.5, enable_color=False, brightness=0.0, contrast=1.0, saturation=1.0, shadow_depth=0, enable_glow=False, glow_blur=0, adaptive_sizing=False)`
- Calls TTS → silence removal → SRT generation → fetch segments from DB → match → build timeline → assemble video → render with preset via existing `_render_with_preset` from library_routes
- Creates a job-like progress dict for polling (reuse the `_generation_progress` pattern from library_routes)
- Returns the final video path

**7. Preview-Only Method**
- Async method `preview_matches(script_text, profile_id, elevenlabs_model="eleven_flash_v2_5")` that does steps 1-4 only (TTS → SRT → match → timeline preview) without rendering
- Returns: {audio_path, audio_duration, srt_content, matches: [...], timeline: [...], unmatched_count, total_phrases}

**Key implementation details:**
- Use Supabase client via lazy initialization (same pattern as segments_routes.py `get_supabase()`)
- Profile-scoped temp directories: `settings.base_dir / "temp" / profile_id / "assembly_xxx"`
- Segment query: fetch from `editai_segments` with profile_id filter, join `editai_source_videos` for file_path
- Import `_render_with_preset` from library_routes (import at function level to avoid circular imports)
- Use dataclasses for MatchResult and TimelineEntry for type safety
- Factory function `get_assembly_service()` returning singleton
- All FFmpeg calls via subprocess.run with capture_output=True
- Encode extracted segments with `-c:v libx264 -preset fast -crf 23 -an -pix_fmt yuv420p` (same as _extend_video_with_segments)
  </action>
  <verify>
`python -c "from app.services.assembly_service import AssemblyService, get_assembly_service; print('Assembly service imported successfully')"` passes without error. Verify the module has match_srt_to_segments, build_timeline, assemble_video, assemble_and_render, and preview_matches methods.
  </verify>
  <done>
Assembly service module exists with all 7 capabilities: TTS generation, SRT generation, keyword matching, timeline building, video assembly, full render orchestration, and preview-only mode. All methods use existing services (ElevenLabsTTSService, SilenceRemover, generate_srt_from_timestamps, _render_with_preset) without duplicating logic.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create assembly API routes and register router</name>
  <files>app/api/assembly_routes.py, app/main.py</files>
  <action>
Create `app/api/assembly_routes.py` with FastAPI router (prefix="/assembly", tags=["assembly"]):

**Pydantic Models:**

```python
class AssemblyPreviewRequest(BaseModel):
    script_text: str                    # The script to process
    elevenlabs_model: str = "eleven_flash_v2_5"

class MatchPreview(BaseModel):
    srt_index: int
    srt_text: str
    srt_start: float
    srt_end: float
    segment_id: Optional[str]
    segment_keywords: List[str]
    matched_keyword: Optional[str]
    confidence: float

class AssemblyPreviewResponse(BaseModel):
    audio_duration: float
    srt_content: str
    matches: List[MatchPreview]
    total_phrases: int
    matched_count: int
    unmatched_count: int

class AssemblyRenderRequest(BaseModel):
    script_text: str
    elevenlabs_model: str = "eleven_flash_v2_5"
    preset_name: str = "TikTok"         # Export preset name
    # Subtitle settings
    font_size: int = 48
    font_family: str = "Montserrat"
    text_color: str = "#FFFFFF"
    outline_color: str = "#000000"
    outline_width: int = 3
    position_y: int = 85
    shadow_depth: int = 0
    enable_glow: bool = False
    glow_blur: int = 0
    adaptive_sizing: bool = False
    # Video filters
    enable_denoise: bool = False
    denoise_strength: float = 2.0
    enable_sharpen: bool = False
    sharpen_amount: float = 0.5
    enable_color: bool = False
    brightness: float = 0.0
    contrast: float = 1.0
    saturation: float = 1.0

class AssemblyRenderResponse(BaseModel):
    job_id: str
    status: str

class AssemblyStatusResponse(BaseModel):
    status: str                          # "processing", "completed", "failed"
    progress: int                        # 0-100
    current_step: str
    final_video_path: Optional[str] = None
    error: Optional[str] = None
```

**Endpoints:**

1. `POST /assembly/preview` — Accepts AssemblyPreviewRequest, calls `assembly_service.preview_matches()`, returns AssemblyPreviewResponse. Auth via `Depends(get_profile_context)`. This does TTS + match but NOT render.

2. `POST /assembly/render` — Accepts AssemblyRenderRequest, starts background render via `BackgroundTasks`, returns job_id immediately. Internally:
   - Generate a UUID job_id
   - Store job status in module-level `_assembly_jobs` dict (same pattern as library_routes `_generation_progress`)
   - Add background task that calls `assembly_service.assemble_and_render()` and updates job status
   - Fetch preset_data from Supabase `editai_export_presets` table by name (or use default dict if not found)
   - Build subtitle_settings dict from request params

3. `GET /assembly/status/{job_id}` — Returns AssemblyStatusResponse from `_assembly_jobs` dict. Public endpoint (no auth needed, job_id is the secret).

**Router Registration in app/main.py:**
- Import: `from app.api.assembly_routes import router as assembly_router`
- Register: `app.include_router(assembly_router, prefix="/api/v1", tags=["Script-to-Video Assembly"])`
- Place after script_router line

**Key patterns to follow:**
- Auth: `profile: ProfileContext = Depends(get_profile_context)` on preview and render endpoints
- Supabase: lazy initialization `get_supabase()` function at module level
- Error handling: HTTPException with appropriate status codes
- Logging: `logger = logging.getLogger(__name__)` with `[Profile {profile.profile_id}]` prefix
- Background tasks: use FastAPI `BackgroundTasks` (not asyncio) consistent with library_routes pattern
  </action>
  <verify>
1. `python -c "from app.api.assembly_routes import router; print(f'Assembly router with {len(router.routes)} routes')"` shows 3 routes.
2. `python -c "from app.main import app; routes = [r.path for r in app.routes]; print([r for r in routes if 'assembly' in r])"` shows assembly routes registered.
3. Start the server briefly with `timeout 5 python run.py 2>&1 || true` and verify no import errors appear in output.
  </verify>
  <done>
Assembly API has 3 endpoints (preview, render, status) registered under /api/v1/assembly. Preview endpoint returns match data without rendering. Render endpoint starts background job and returns job_id. Status endpoint returns progress. All endpoints follow existing auth and error handling patterns.
  </done>
</task>

</tasks>

<verification>
1. Assembly service imports cleanly: `python -c "from app.services.assembly_service import AssemblyService"`
2. Assembly routes import cleanly: `python -c "from app.api.assembly_routes import router"`
3. Main app starts without import errors: `timeout 5 python run.py 2>&1 | head -20`
4. API docs show assembly endpoints: start server, check http://localhost:8000/docs for /api/v1/assembly/* endpoints
</verification>

<success_criteria>
- Assembly service has matching engine that compares SRT phrases against segment keywords with confidence scoring
- Timeline builder arranges segments to cover full audio duration
- Preview endpoint returns match data without triggering expensive render
- Render endpoint starts background job using existing _render_with_preset pipeline
- Silence removal is applied to TTS audio before timeline calculation
- All code reuses existing services (ElevenLabsTTSService, SilenceRemover, generate_srt_from_timestamps, _render_with_preset)
</success_criteria>

<output>
After completion, create `.planning/phases/15-script-to-video-assembly/15-01-SUMMARY.md`
</output>
