---
phase: 07-platform-export-presets
plan: 02
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - app/api/library_routes.py
autonomous: true

must_haves:
  truths:
    - "Exported TikTok video uses CRF 20 encoding"
    - "Exported Reels video uses CRF 18 encoding"
    - "Exported videos have -g 60 keyframe interval in FFmpeg command"
    - "Exported videos have 192k audio bitrate"
    - "GPU encoding uses NVENC with equivalent quality parameters"
  artifacts:
    - path: "app/api/library_routes.py"
      provides: "Updated _render_with_preset integrating EncodingPreset.to_ffmpeg_params()"
      contains: '"-g"'
  key_links:
    - from: "app/api/library_routes.py"
      to: "app/services/encoding_presets.py"
      via: "get_preset import and to_ffmpeg_params() call"
      pattern: "get_preset\\(.*\\)\\.to_ffmpeg_params"
---

<objective>
Integrate the encoding presets service into the final render pipeline, using EncodingPreset.to_ffmpeg_params() to generate FFmpeg parameters with keyframe controls.

Purpose: Ensures exported videos use platform-optimized encoding with proper keyframe intervals for platform recompression compatibility.

Output: Updated `_render_with_preset` function using EncodingPreset.to_ffmpeg_params() for encoding parameters
</objective>

<execution_context>
@/home/ukfdb/.claude/get-shit-done/workflows/execute-plan.md
@/home/ukfdb/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-platform-export-presets/07-RESEARCH.md
@.planning/phases/07-platform-export-presets/07-01-SUMMARY.md

# Current implementation to modify
@app/api/library_routes.py
@app/services/encoding_presets.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update _render_with_preset to use EncodingPreset.to_ffmpeg_params()</name>
  <files>app/api/library_routes.py</files>
  <action>
Modify `_render_with_preset` function in `app/api/library_routes.py` to use the EncodingPreset model's to_ffmpeg_params() method:

1. **Add import at top of file** (after existing imports):
   ```python
   from app.services.encoding_presets import get_preset, EncodingPreset
   ```

2. **In _render_with_preset function**, replace the hardcoded encoding parameter building with EncodingPreset integration:

   **Before the FFmpeg command construction, get the EncodingPreset object:**
   ```python
   # Get encoding preset from code-first presets module
   # Map database preset name to platform key (instagram_reels -> reels, etc.)
   platform_key = preset.get("name", "generic").replace("instagram_", "").replace("_optimized", "")
   encoding_preset = get_preset(platform_key)

   # Get FFmpeg encoding params from the EncodingPreset model
   # This includes: -g, -keyint_min, -sc_threshold, -bf, audio params, -pix_fmt
   use_gpu = False  # Set based on available GPU detection or config
   encoding_params = encoding_preset.to_ffmpeg_params(use_gpu=use_gpu)

   logger.info(f"Encoding with preset: {encoding_preset.name} (CRF={encoding_preset.crf}, GOP={encoding_preset.gop_size}, audio={encoding_preset.audio_bitrate})")
   ```

3. **Replace the video encoding section** (around line 2379-2400) to use encoding_params:

   Instead of manually building params like:
   ```python
   cmd.extend(["-c:v", "libx264", "-crf", "18", ...])  # OLD - hardcoded
   ```

   Use the preset-generated params:
   ```python
   cmd.extend(encoding_params)  # NEW - from EncodingPreset.to_ffmpeg_params()
   ```

   **Important:** The to_ffmpeg_params() method returns a complete list including:
   - Video codec (-c:v)
   - CRF or CQ (-crf or -cq for GPU)
   - Preset (-preset)
   - Keyframe controls (-g, -keyint_min, -sc_threshold, -bf)
   - Audio params (-c:a, -b:a, -ar, -ac)
   - Pixel format (-pix_fmt yuv420p, -sar 1:1)

   You may need to adjust the cmd building to:
   - Keep existing filter chain logic (scale, crop, subtitles) BEFORE encoding_params
   - Keep existing input/output file handling
   - Remove any duplicate audio params if already in encoding_params

4. **Preserve existing audio bitrate if higher than 192k:**
   If the database preset has audio_bitrate = "320k" (higher than 192k), preserve it:
   ```python
   # If database preset has higher audio bitrate, use that
   db_audio_bitrate = preset.get("audio_bitrate", "192k")
   if db_audio_bitrate and int(db_audio_bitrate.replace("k", "")) > 192:
       # Override the 192k default with higher value
       encoding_params = [p if p != "192k" else db_audio_bitrate for p in encoding_params]
   ```

Important:
- Keep existing filter chain logic (scale, crop, subtitles) BEFORE encoding params
- Keep existing silent audio handling
- The to_ffmpeg_params() output is a flat list of strings, extend cmd with it
  </action>
  <verify>
1. Check import exists:
```bash
grep -n "from app.services.encoding_presets import" "/mnt/c/OBSID SRL/n8n/edit_factory/app/api/library_routes.py" | head -2
```

2. Check to_ffmpeg_params is called:
```bash
grep -n "to_ffmpeg_params" "/mnt/c/OBSID SRL/n8n/edit_factory/app/api/library_routes.py" | head -2
```

3. Check keyframe params will be in output:
```bash
grep -n '"-g"' "/mnt/c/OBSID SRL/n8n/edit_factory/app/api/library_routes.py" | head -5
```

4. Verify Python syntax:
```bash
cd "/mnt/c/OBSID SRL/n8n/edit_factory" && python -c "from app.api import library_routes; print('Import OK')"
```
  </verify>
  <done>
- _render_with_preset uses get_preset() to get EncodingPreset object
- to_ffmpeg_params() is called to generate encoding parameters
- FFmpeg command includes -g and -keyint_min parameters
- Audio bitrate preserves existing higher values (320k) if present
  </done>
</task>

<task type="auto">
  <name>Task 2: Update database presets with keyframe columns (deterministic migration)</name>
  <files>N/A - Database operation via Supabase Python client</files>
  <action>
Update the existing export presets in the database with keyframe parameters using a deterministic approach:

**Step 1: Check existing schema and add columns if needed**

Run this Python script to inspect schema and add missing columns:
```python
import os
from dotenv import load_dotenv
load_dotenv()
from supabase import create_client

supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))

# 1. Check current schema by fetching one row
result = supabase.table("editai_export_presets").select("*").limit(1).execute()
if result.data:
    existing_columns = set(result.data[0].keys())
    print(f"Existing columns: {existing_columns}")
else:
    print("No existing presets found - table may be empty")
    existing_columns = set()

# Required columns for keyframe support
required_columns = {"video_preset", "gop_size", "keyint_min"}
missing_columns = required_columns - existing_columns

if missing_columns:
    print(f"Missing columns (need SQL migration): {missing_columns}")
    print("Run this SQL in Supabase Dashboard -> SQL Editor:")
    print("""
ALTER TABLE editai_export_presets
ADD COLUMN IF NOT EXISTS video_preset TEXT DEFAULT 'medium',
ADD COLUMN IF NOT EXISTS gop_size INTEGER DEFAULT 60,
ADD COLUMN IF NOT EXISTS keyint_min INTEGER DEFAULT 60;
    """)
else:
    print("All required columns exist - proceeding with upsert")
```

**Step 2: Upsert presets with new values**

After confirming columns exist (or adding them via SQL), run upsert:
```python
import os
from dotenv import load_dotenv
load_dotenv()
from supabase import create_client

supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))

presets_to_upsert = [
    {
        "name": "tiktok",
        "display_name": "TikTok",
        "crf": 20,
        "video_preset": "medium",
        "gop_size": 60,
        "keyint_min": 60,
        "audio_bitrate": "192k"
    },
    {
        "name": "instagram_reels",
        "display_name": "Instagram Reels",
        "crf": 18,
        "video_preset": "slow",
        "gop_size": 60,
        "keyint_min": 60,
        "audio_bitrate": "192k",
        "is_default": True
    },
    {
        "name": "youtube_shorts",
        "display_name": "YouTube Shorts",
        "crf": 18,
        "video_preset": "slow",
        "gop_size": 60,
        "keyint_min": 60,
        "audio_bitrate": "192k"
    },
    {
        "name": "generic",
        "display_name": "Generic High Quality",
        "crf": 20,
        "video_preset": "medium",
        "gop_size": 60,
        "keyint_min": 60,
        "audio_bitrate": "192k"
    }
]

for preset in presets_to_upsert:
    result = supabase.table("editai_export_presets").upsert(
        preset,
        on_conflict="name"
    ).execute()
    print(f"Upserted preset: {preset['name']}")

print("Database presets updated successfully")
```

**Step 3: Verify the update**
```python
result = supabase.table("editai_export_presets").select("name,crf,audio_bitrate,gop_size,keyint_min,video_preset").execute()
for p in result.data:
    print(f"{p['name']}: CRF={p.get('crf')}, audio={p.get('audio_bitrate')}, gop={p.get('gop_size')}, preset={p.get('video_preset')}")
```

Important:
- This is a 3-step deterministic process: check -> migrate if needed -> upsert
- The upsert handles both insert (new preset) and update (existing preset) cases
- Preserves existing presets like "instagram_reels" (the current default)
  </action>
  <verify>
Query presets to verify all fields:
```bash
cd "/mnt/c/OBSID SRL/n8n/edit_factory" && python -c "
import os
from dotenv import load_dotenv
load_dotenv()
from supabase import create_client

supabase = create_client(os.getenv('SUPABASE_URL'), os.getenv('SUPABASE_KEY'))
result = supabase.table('editai_export_presets').select('name,crf,audio_bitrate,gop_size,keyint_min,video_preset').execute()
for p in result.data:
    print(f\"{p['name']}: CRF={p.get('crf')}, audio={p.get('audio_bitrate')}, gop={p.get('gop_size')}, preset={p.get('video_preset')}\")
"
```

Expected output:
- All presets show CRF 18-20
- All presets show audio_bitrate=192k
- All presets show gop_size=60
- All presets show video_preset (medium or slow)
  </verify>
  <done>
- Database schema has gop_size, keyint_min, video_preset columns
- All 4 presets upserted with correct values
- Audio bitrate is 192k across all presets
- CRF values are 18-20 (professional range)
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify end-to-end encoding integration</name>
  <files>N/A - Integration verification</files>
  <action>
Verify the complete integration works:

1. **Test preset API endpoint returns presets with keyframe params:**
```bash
cd "/mnt/c/OBSID SRL/n8n/edit_factory" && python -c "
import os
from dotenv import load_dotenv
load_dotenv()
from supabase import create_client

supabase = create_client(os.getenv('SUPABASE_URL'), os.getenv('SUPABASE_KEY'))
result = supabase.table('editai_export_presets').select('*').execute()
for p in result.data:
    has_keyframe = 'gop_size' in p and p['gop_size'] is not None
    print(f\"{p['name']}: has_keyframe_params={has_keyframe}, gop={p.get('gop_size')}\")
"
```

2. **Verify EncodingPreset.to_ffmpeg_params() generates correct command:**
```bash
cd "/mnt/c/OBSID SRL/n8n/edit_factory" && python -c "
from app.services.encoding_presets import get_preset

# Test each platform
for platform in ['tiktok', 'reels', 'youtube_shorts', 'generic']:
    preset = get_preset(platform)
    params = preset.to_ffmpeg_params(use_gpu=False)

    # Verify critical params are present
    assert '-g' in params, f'{platform}: Missing -g'
    assert '-keyint_min' in params, f'{platform}: Missing -keyint_min'
    assert '192k' in params, f'{platform}: Missing 192k audio'
    assert '-crf' in params, f'{platform}: Missing -crf'

    crf_idx = params.index('-crf')
    crf_val = params[crf_idx + 1]
    print(f'{platform}: CRF={crf_val}, has -g=OK, has -keyint_min=OK, audio=192k')

print('All presets validated successfully')
"
```

3. **Verify library_routes imports and uses the preset module:**
```bash
cd "/mnt/c/OBSID SRL/n8n/edit_factory" && python -c "
from app.api.library_routes import router
from app.services.encoding_presets import get_preset, EncodingPreset
print('Both modules import successfully')
print(f'get_preset available: {callable(get_preset)}')
print(f'EncodingPreset available: {EncodingPreset is not None}')
"
```
  </action>
  <verify>
All verification commands should complete without errors:
1. Database presets have keyframe params
2. EncodingPreset.to_ffmpeg_params() generates correct params for all platforms
3. library_routes and encoding_presets modules both import successfully
  </verify>
  <done>
- Database presets verified with keyframe columns
- EncodingPreset.to_ffmpeg_params() generates correct FFmpeg parameters
- Integration between library_routes and encoding_presets is working
  </done>
</task>

</tasks>

<verification>
1. `_render_with_preset` calls EncodingPreset.to_ffmpeg_params() for encoding params
2. Database presets have correct CRF (18-20), audio (192k), keyframe (60) values
3. FFmpeg parameters include -g, -keyint_min from the preset module
</verification>

<success_criteria>
- ENC-01 satisfied: Platform-specific presets applied during export
- ENC-02 satisfied: CRF 18-20, preset medium/slow used in render
- ENC-03 satisfied: Keyframe controls (-g 60, -keyint_min 60) added to FFmpeg command
- ENC-04 satisfied: Audio encoded at 192k bitrate (or higher if already set)
- ENC-05 implicitly satisfied: Presets are data-driven (code-first + database fallbacks)
</success_criteria>

<uat_note>
**Manual UAT Required:** Actual platform upload validation (uploading to TikTok/Reels/YouTube and verifying no recompression artifacts) is out of scope for automated testing. This should be verified manually by:
1. Exporting a video with each preset
2. Uploading to the respective platform
3. Verifying video quality is maintained after platform processing
</uat_note>

<output>
After completion, create `.planning/phases/07-platform-export-presets/07-02-SUMMARY.md`
</output>
