---
phase: 17-feed-foundation
plan: 01
type: execute
wave: 2
depends_on: ["17-02"]
files_modified:
  - supabase/migrations/013_create_product_tables.sql
  - app/services/feed_parser.py
  - app/api/feed_routes.py
  - app/main.py
  # imports from (created by 17-02):
  # - app/services/image_fetcher.py
autonomous: true
requirements:
  - FEED-01
  - FEED-07

must_haves:
  truths:
    - "User can add a Google Shopping XML feed URL and trigger a sync via POST /api/v1/feeds"
    - "Feed sync parses 10k products with streaming XML (lxml iterparse + elem.clear) without memory spikes"
    - "Synced products are stored in Supabase products table with title, price, sale_price, brand, product_type, image_link, product_url populated"
    - "HTML tags and entities in product text fields are stripped by clean_product_text() before storage"
    - "Re-syncing a feed upserts (no duplicate products) via ON CONFLICT feed_id,external_id"
  artifacts:
    - path: "supabase/migrations/013_create_product_tables.sql"
      provides: "product_feeds and products tables with indexes and RLS"
      contains: "CREATE TABLE.*product_feeds"
    - path: "app/services/feed_parser.py"
      provides: "Streaming XML parser, clean_product_text, parse_price, upsert_products"
      exports: ["parse_feed_xml", "clean_product_text", "parse_price", "upsert_products"]
    - path: "app/api/feed_routes.py"
      provides: "Feed CRUD + sync endpoint + products listing"
      exports: ["router"]
  key_links:
    - from: "app/api/feed_routes.py"
      to: "app/services/feed_parser.py"
      via: "import parse_feed_xml, upsert_products"
      pattern: "from app\\.services\\.feed_parser import"
    - from: "app/api/feed_routes.py"
      to: "supabase products table"
      via: "supabase.table('products')"
      pattern: "table\\('products'\\)"
    - from: "app/main.py"
      to: "app/api/feed_routes.py"
      via: "app.include_router(feed_router)"
      pattern: "include_router.*feed"
    - from: "app/api/feed_routes.py"
      to: "app/services/image_fetcher.py"
      via: "sync background task calls download_product_images + update_local_image_paths"
      pattern: "from app\\.services\\.image_fetcher import"
---

<objective>
Create the product data foundation: DB tables for feed URLs and products, a streaming XML parser for Google Shopping feeds, and API routes for feed CRUD and sync.

Purpose: This is the data backbone for v5 — all downstream phases (product browser, video generation, batch) depend on products being parsed and stored correctly.
Output: Working /api/v1/feeds endpoints, feed_parser.py service, Supabase migration for product_feeds + products tables.
</objective>

<execution_context>
@/home/ukfdb/.claude/get-shit-done/workflows/execute-plan.md
@/home/ukfdb/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-feed-foundation/17-RESEARCH.md
@app/api/library_routes.py (reference for Supabase pattern, BackgroundTasks, get_supabase)
@app/api/auth.py (reference for get_profile_context dependency)
@app/main.py (reference for router mounting pattern)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Database migration for product_feeds and products tables</name>
  <files>supabase/migrations/013_create_product_tables.sql</files>
  <action>
Create migration `013_create_product_tables.sql` with:

1. **product_feeds table:**
   - `id UUID PRIMARY KEY DEFAULT gen_random_uuid()`
   - `profile_id UUID NOT NULL REFERENCES public.profiles(id) ON DELETE CASCADE`
   - `name TEXT NOT NULL`
   - `feed_url TEXT NOT NULL`
   - `last_synced_at TIMESTAMPTZ`
   - `product_count INTEGER DEFAULT 0`
   - `sync_status TEXT DEFAULT 'idle'` (values: idle, syncing, error)
   - `sync_error TEXT`
   - `created_at TIMESTAMPTZ DEFAULT NOW()`
   - `updated_at TIMESTAMPTZ DEFAULT NOW()`

2. **products table:**
   - `id UUID PRIMARY KEY DEFAULT gen_random_uuid()`
   - `feed_id UUID NOT NULL REFERENCES public.product_feeds(id) ON DELETE CASCADE`
   - `external_id TEXT NOT NULL` (g:id from feed)
   - `title TEXT NOT NULL`
   - `brand TEXT`
   - `product_type TEXT`
   - `price FLOAT` (parsed float for SQL filtering)
   - `sale_price FLOAT`
   - `raw_price_str TEXT` (original "249.99 RON" for display)
   - `raw_sale_price_str TEXT`
   - `is_on_sale BOOLEAN DEFAULT FALSE`
   - `image_link TEXT`
   - `local_image_path TEXT`
   - `product_url TEXT`
   - `description TEXT`
   - `created_at TIMESTAMPTZ DEFAULT NOW()`
   - `updated_at TIMESTAMPTZ DEFAULT NOW()`
   - `UNIQUE(feed_id, external_id)` (enables ON CONFLICT upsert)

3. **Indexes** (for Phase 19 product browser performance):
   - `idx_products_feed_id ON products(feed_id)`
   - `idx_products_brand ON products(feed_id, brand)`
   - `idx_products_product_type ON products(feed_id, product_type)`
   - `idx_products_is_on_sale ON products(feed_id, is_on_sale)`
   - `idx_products_title_gin ON products USING gin(to_tsvector('romanian', title))`

4. **RLS policies** — follow existing pattern from migration 011 (elevenlabs_accounts):
   - Enable RLS on both tables
   - `product_feeds`: SELECT/INSERT/UPDATE/DELETE for authenticated users where `profile_id IN (SELECT id FROM profiles WHERE user_id = auth.uid())`
   - `products`: SELECT/INSERT/UPDATE/DELETE for authenticated users where `feed_id IN (SELECT id FROM product_feeds WHERE profile_id IN (SELECT id FROM profiles WHERE user_id = auth.uid()))`

5. **updated_at triggers** — reuse existing `handle_updated_at` function:
   - `CREATE TRIGGER ... BEFORE UPDATE ON product_feeds FOR EACH ROW EXECUTE FUNCTION handle_updated_at()`
   - Same for products table

Also apply migration to Supabase using the MCP tool `apply_migration`.
  </action>
  <verify>
Run `mcp__supabase__execute_sql` with `SELECT table_name FROM information_schema.tables WHERE table_schema='public' AND table_name IN ('product_feeds','products')` — both tables exist.
Run `mcp__supabase__execute_sql` with `SELECT indexname FROM pg_indexes WHERE tablename='products'` — all 5 indexes present.
  </verify>
  <done>product_feeds and products tables exist in Supabase with correct columns, indexes, RLS policies, and updated_at triggers. Migration file saved locally and applied remotely.</done>
</task>

<task type="auto">
  <name>Task 2: Feed parser service + feed API routes</name>
  <files>app/services/feed_parser.py, app/api/feed_routes.py, app/main.py</files>
  <action>
**Create `app/services/feed_parser.py`** with these functions:

1. `clean_product_text(text: str) -> str` — Strip HTML tags with `re.sub(r'<[^>]+>', '', text)`, then decode entities with `html.unescape()`. Return stripped text. Return empty string for falsy input.

2. `parse_price(price_str: str) -> float | None` — Extract numeric value from strings like "249.99 RON", "249,99 RON", "1.249,99 RON". Handle Romanian comma-as-decimal and dot-as-thousands. Return None for empty/invalid input.

3. `parse_feed_xml(xml_bytes: bytes) -> list[dict]` — Stream-parse Google Shopping XML using `lxml.etree.iterparse` with `io.BytesIO(xml_bytes)`. Use namespace URI `http://base.google.com/ns/1.0` (not g: prefix). Extract: external_id, title, price, sale_price, raw_price_str, raw_sale_price_str, brand, product_type, image_link, product_url, description, is_on_sale (derived: sale_price < price when both exist). **CRITICAL memory safety:** call `elem.clear()` then `while elem.getprevious() is not None: del elem.getparent()[0]` after each item. Detect feed format: if `<rss` in first 500 bytes, use `tag='item'`; otherwise use `tag='{http://www.w3.org/2005/Atom}entry'`.

4. `upsert_products(supabase, products: list[dict], feed_id: str)` — Upsert in batches of 500 using `supabase.table('products').upsert(batch, on_conflict='feed_id,external_id').execute()`. Add `feed_id` to each product dict before upserting.

**Create `app/api/feed_routes.py`** with router prefix `/feeds`:

1. `POST /feeds` — Create a feed. Accept JSON `{name, feed_url}`. Insert into `product_feeds` with profile_id from `get_profile_context`. Return created feed.

2. `GET /feeds` — List feeds for current profile. Select from `product_feeds` where profile_id matches.

3. `GET /feeds/{feed_id}` — Get single feed with product_count.

4. `DELETE /feeds/{feed_id}` — Delete feed (CASCADE deletes products).

5. `POST /feeds/{feed_id}/sync` — Trigger sync as BackgroundTask. Set `sync_status='syncing'` immediately. Background task:
   a. Download XML with httpx (set User-Agent header per research)
   b. Call `parse_feed_xml` to extract products
   c. Call `upsert_products` to persist to Supabase
   d. **Download product images:** Import `download_product_images` and `update_local_image_paths` from `app.services.image_fetcher`. Call `asyncio.run(download_product_images(products, Path(settings.output_dir) / "product_images" / feed_id, feed_id))` to download images in parallel. Then call `update_local_image_paths(supabase, image_map, feed_id)` to set `local_image_path` on each product row. This ensures Phase 18 compositor always has local images available after sync.
   e. Update feed status to idle with product_count and last_synced_at.
   On error at any step: set `sync_status='error'` with error message.

6. `GET /feeds/{feed_id}/products` — List products for a feed. Support query params: `page` (default 1), `page_size` (default 50). Return products with pagination metadata. This is a basic listing — advanced filters (search, brand, category, on-sale) will be added in Phase 19.

Auth: All routes use `Depends(get_profile_context)` following existing pattern from library_routes.py.

**Update `app/main.py`** — Import and mount feed_routes router:
```python
from app.api.feed_routes import router as feed_router
app.include_router(feed_router, prefix="/api/v1", tags=["feeds"])
```
Mount with `prefix="/api/v1"` (NOT `/api/v1/feeds`) — the router's own `prefix="/feeds"` handles the rest. This matches the established pattern used by all other routers (library_router, segments_router, etc.).
  </action>
  <verify>
Backend starts without import errors: `cd "/mnt/c/OBSID SRL/n8n/edit_factory" && python -c "from app.api.feed_routes import router; from app.services.feed_parser import parse_feed_xml, clean_product_text, parse_price, upsert_products; print('OK')"`.
Verify clean_product_text: `python -c "from app.services.feed_parser import clean_product_text; assert clean_product_text('&lt;b&gt;Pește&lt;/b&gt; &amp; chips') == 'Pește & chips'; print('OK')"`.
Verify parse_price: `python -c "from app.services.feed_parser import parse_price; assert parse_price('249.99 RON') == 249.99; assert parse_price('1.249,99 RON') == 1249.99; print('OK')"`.
  </verify>
  <done>feed_parser.py exports parse_feed_xml (streaming, memory-safe), clean_product_text, parse_price, upsert_products. feed_routes.py provides /feeds CRUD + /feeds/{id}/sync + /feeds/{id}/products endpoints. Router mounted in main.py. All imports resolve, helper functions produce correct output.</done>
</task>

</tasks>

<verification>
1. Migration 013 applied — product_feeds and products tables exist with all columns, indexes, RLS, triggers
2. `python -c "from app.api.feed_routes import router"` succeeds
3. `clean_product_text` strips HTML and decodes entities correctly
4. `parse_price` handles "249.99 RON", "249,99 RON", "1.249,99 RON" formats
5. `parse_feed_xml` uses iterparse with elem.clear() — grep for `elem.clear()` in feed_parser.py
6. `upsert_products` chunks at 500 with on_conflict
7. Feed routes use `Depends(get_profile_context)` for auth
8. Sync background task imports and calls `download_product_images` + `update_local_image_paths` from image_fetcher — grep for `from app.services.image_fetcher import` in feed_routes.py
</verification>

<success_criteria>
- product_feeds and products tables live in Supabase with correct schema
- Feed CRUD API works (create, list, get, delete feeds)
- Sync endpoint triggers background XML parse and product upsert
- Parser handles Google Shopping RSS and Atom formats
- Parser uses streaming iterparse with memory cleanup for 10k safety
- Products listing endpoint returns paginated results
</success_criteria>

<output>
After completion, create `.planning/phases/17-feed-foundation/17-01-SUMMARY.md`
</output>
