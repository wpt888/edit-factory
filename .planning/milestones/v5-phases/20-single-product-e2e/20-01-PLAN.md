---
phase: 20-single-product-e2e
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/api/product_generate_routes.py
  - app/main.py
autonomous: true
requirements:
  - TTS-01
  - TTS-02
  - TTS-03
  - TTS-04
  - BATCH-01
  - BATCH-05
  - OUT-01
  - OUT-02
  - OUT-03

must_haves:
  truths:
    - "POST /api/v1/products/{product_id}/generate returns a job_id immediately"
    - "Background task generates TTS audio from quick-mode template text (title + price + brand)"
    - "Background task generates TTS audio from elaborate-mode AI script via ScriptGenerator"
    - "Background task produces synced SRT subtitles when using ElevenLabs (skips subtitles for Edge TTS)"
    - "Background task runs compositor, then _render_with_preset for audio mux + encoding preset + filters + subtitles"
    - "Completed video is inserted into editai_projects + editai_clips tables so it appears in /librarie"
    - "Job status updates at each pipeline stage so frontend polling shows real-time progress"
  artifacts:
    - path: "app/api/product_generate_routes.py"
      provides: "Product video generation endpoint and background task"
      contains: "generate_product_video"
    - path: "app/main.py"
      provides: "Router registration"
      contains: "product_generate"
  key_links:
    - from: "app/api/product_generate_routes.py"
      to: "app/services/product_video_compositor.py"
      via: "compose_product_video() call in background task"
      pattern: "compose_product_video"
    - from: "app/api/product_generate_routes.py"
      to: "app/services/tts/factory.py"
      via: "get_tts_service() for TTS audio generation"
      pattern: "get_tts_service"
    - from: "app/api/product_generate_routes.py"
      to: "app/services/tts_subtitle_generator.py"
      via: "generate_srt_from_timestamps() for ElevenLabs subtitles"
      pattern: "generate_srt_from_timestamps"
    - from: "app/api/product_generate_routes.py"
      to: "app/api/library_routes.py"
      via: "_render_with_preset() import for final render"
      pattern: "_render_with_preset"
    - from: "app/api/product_generate_routes.py"
      to: "app/services/job_storage.py"
      via: "create_job/update_job for status tracking"
      pattern: "get_job_storage"
---

<objective>
Build the backend endpoint and full background task pipeline for single-product video generation. One POST endpoint dispatches a BackgroundTask that: fetches the product, resolves the image, generates TTS voiceover (quick template or elaborate AI), generates SRT subtitles from timestamps (ElevenLabs only), runs the Phase 18 compositor for silent video with overlays, then runs _render_with_preset for audio mux + encoding preset + normalization + filters + subtitle burn, and finally inserts the result into editai_projects + editai_clips so the video appears in the library.

Purpose: This is the core pipeline that makes single product video generation work. The frontend (Plan 02) is just a UI over this endpoint.
Output: `app/api/product_generate_routes.py` with working endpoint and background task, registered in `app/main.py`.
</objective>

<execution_context>
@/home/ukfdb/.claude/get-shit-done/workflows/execute-plan.md
@/home/ukfdb/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-single-product-e2e/20-RESEARCH.md

# Key source files to read before implementing:
@app/api/product_routes.py (existing product listing routes — see router prefix pattern)
@app/services/product_video_compositor.py (compose_product_video signature + CompositorConfig)
@app/services/tts/factory.py (get_tts_service signature)
@app/services/tts/elevenlabs.py (generate_audio_with_timestamps method)
@app/services/tts/edge.py (generate_audio — no timestamps)
@app/services/tts_subtitle_generator.py (generate_srt_from_timestamps)
@app/services/script_generator.py (ScriptGenerator.generate_scripts)
@app/services/job_storage.py (create_job/update_job pattern)
@app/api/library_routes.py (lines 1784-1900 for _render_final_clip_task pattern, lines 2585-2700 for _render_with_preset)
@app/api/auth.py (get_profile_context pattern)
@app/main.py (router registration pattern)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create product_generate_routes.py with POST endpoint and request model</name>
  <files>app/api/product_generate_routes.py, app/main.py</files>
  <action>
Create `app/api/product_generate_routes.py` with:

1. **Pydantic request model** `ProductGenerateRequest`:
   - `voiceover_mode`: str, either `"quick"` or `"elaborate"` (default `"quick"`)
   - `tts_provider`: str, either `"edge"` or `"elevenlabs"` (default `"edge"` per roadmap decision)
   - `voice_id`: Optional[str] = None (override; otherwise use profile/default)
   - `ai_provider`: str, either `"gemini"` or `"claude"` (default `"gemini"`, only used in elaborate mode)
   - `duration_s`: int = 30 (video duration: 15, 30, 45, or 60)
   - `encoding_preset`: str = "tiktok" (one of: tiktok, reels, youtube_shorts)
   - `voiceover_template`: str = "{title}. {brand}. Pret: {price} lei." (template for quick mode)
   - `cta_text`: str = "Comanda acum!"
   - `enable_denoise`: bool = False
   - `enable_sharpen`: bool = False
   - `enable_color_correction`: bool = False

2. **Router** with prefix matching existing product_routes pattern. Use `APIRouter(tags=["product-video"])`.

3. **POST `/products/{product_id}/generate` endpoint**:
   - Depends on `get_profile_context` for auth
   - Accepts `ProductGenerateRequest` as JSON body
   - Fetches product row from Supabase `products` table to validate it exists — return 404 if not found
   - Creates job via `get_job_storage().create_job()` with `job_type="product_video"`, `status="pending"`, `progress="0"`
   - Dispatches `_generate_product_video_task` as `BackgroundTask`
   - Returns `{"job_id": job_id, "status": "pending"}` immediately

4. **Register router** in `app/main.py`: import and `app.include_router(product_generate_router, prefix="/api/v1")`. Follow the exact pattern used for other routers (product_routes, assembly_routes, etc.).

Important: The endpoint path should be `/products/{product_id}/generate` (not nested under feeds). Product ID is the UUID from the `products` table.
  </action>
  <verify>
Run `python -c "from app.api.product_generate_routes import router"` — should import without errors.
Run `python -c "from app.main import app; routes = [r.path for r in app.routes]; assert '/api/v1/products/{product_id}/generate' in routes or any('generate' in r for r in routes)"` — confirms route registered.
  </verify>
  <done>
POST /api/v1/products/{product_id}/generate endpoint exists, accepts ProductGenerateRequest JSON body, validates product exists, creates a job, dispatches background task, and returns job_id. Router is registered in app/main.py.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement full background task pipeline — TTS, subtitles, compositor, render, library insert</name>
  <files>app/api/product_generate_routes.py</files>
  <action>
Implement `_generate_product_video_task` async function in `product_generate_routes.py`. This is the core pipeline. Follow the exact pattern from `_render_final_clip_task` in `library_routes.py` for job status updates and error handling.

**Pipeline stages with progress updates:**

**Stage 1 (0-10%): Setup**
- Update job: `status="processing"`, `progress="5"`
- Fetch product row from Supabase `products` table
- Resolve image: check `local_image_path` exists on disk; if NULL or missing, attempt re-download from `image_link` using the same `download_image` function from `image_fetcher.py`; if still fails, set job to `failed` with error "Product image not available"
- Create temp directory for intermediate files

**Stage 2 (10-40%): TTS Voiceover Generation**
- Update job: `progress="10"`
- **Quick mode** (`voiceover_mode == "quick"`):
  - Build voiceover text from template: `voiceover_template.format(title=product["title"], brand=product.get("brand", ""), price=product.get("raw_sale_price_str") or product.get("raw_price_str") or "")`
  - No AI call needed
- **Elaborate mode** (`voiceover_mode == "elaborate"`):
  - Use `ScriptGenerator.generate_scripts(idea=product["title"], context=product.get("description", ""), keywords=[], variant_count=1, provider=ai_provider)`
  - Extract `scripts[0]` as voiceover text
  - Run in executor if needed (sync call): `await asyncio.get_event_loop().run_in_executor(None, lambda: generator.generate_scripts(...))`
- Update job: `progress="25"`
- **TTS synthesis:**
  - If `tts_provider == "elevenlabs"`: instantiate `ElevenLabsTTSService` directly, call `generate_audio_with_timestamps()` to get both audio and timestamps. Use `voice_id` from request or fall back to service default.
  - If `tts_provider == "edge"`: use `get_tts_service("edge", profile_id=profile_id)`, call `generate_audio()`. Default voice to `"ro-RO-EmilNeural"` if `voice_id` is None (Romanian products). No timestamps available.
  - Output path: `temp_dir / f"tts_{job_id}.mp3"`
- Update job: `progress="40"`

**Stage 3 (40-50%): Subtitle Generation**
- If `tts_provider == "elevenlabs"` and timestamps are available:
  - Call `generate_srt_from_timestamps(timestamps)` to get SRT string
  - Write SRT to `temp_dir / f"subtitles_{job_id}.srt"`
- If Edge TTS or no timestamps: `srt_path = None` (no subtitles)
- Update job: `progress="50"`

**Stage 4 (50-70%): Video Composition (silent)**
- Build `CompositorConfig` from product data + request params:
  - `image_path`: resolved local image path
  - `output_path`: `temp_dir / f"composed_{job_id}.mp4"`
  - `duration_s`: from request
  - `product_name`, `brand`, `price_str`, `sale_price_str`: from product row
  - `cta_text`: from request
  - `is_on_sale`: True if product has `sale_price` < `price`
- Call `compose_product_video(config)` — this produces silent MP4 with Ken Burns + text overlays
- Update job: `progress="70"`

**Stage 5 (70-90%): Final Render with Preset**
- Import `_render_with_preset` from `app.api.library_routes` (direct import per research recommendation)
- Build render params matching what `_render_with_preset` expects:
  - Input video: composed video path
  - Audio: TTS mp3 path
  - SRT path: if available
  - Encoding preset: `get_preset(request.encoding_preset)`
  - Video filters: denoise/sharpen/color from request booleans
  - Subtitle settings: hardcoded defaults (font_size=48, color=white, outline=black, position_y=85%)
  - Output path: `output_dir / f"product_{product_id}_{job_id}.mp4"` where output_dir is a persistent directory like `output/product_videos/`
- Call `_render_with_preset()` — this handles audio mux, -14 LUFS normalization (OUT-02), encoding preset (OUT-01), video filters (OUT-03), and subtitle burn
- Update job: `progress="90"`

**Stage 6 (90-100%): Library Insert**
- Get Supabase client via `get_supabase()` from library_routes or config
- Insert `editai_projects` row: `name=f"[Product] {product['title'][:50]}"`, `profile_id`, `status="completed"`, `target_duration=duration_s`
- Insert `editai_clips` row: `project_id`, `profile_id`, `raw_video_path=composed_path`, `final_video_path=final_path`, `final_status="completed"`, `variant_index=0`, `is_selected=True`
- Update job: `status="completed"`, `progress="100"`, add `result={"clip_id": clip_id, "project_id": project_id, "video_path": str(final_path)}` in job data

**Error handling:**
- Wrap entire function in try/except
- On ANY exception: `job_storage.update_job(job_id, {"status": "failed", "error": str(e)})`, log traceback
- Specific early exits: missing product image, empty voiceover text, compositor failure

**Important implementation notes (from research pitfalls):**
- Use `generate_audio_with_timestamps()` (NOT `generate_audio()`) for ElevenLabs — otherwise timestamps are None
- Edge TTS default voice for Romanian: `"ro-RO-EmilNeural"` when voice_id is None
- Job `progress` field is a STRING like `"25"` (not int) — match existing convention
- `_render_with_preset` is private by convention but importable — use `from app.api.library_routes import _render_with_preset`
- `ScriptGenerator.generate_scripts()` is synchronous — wrap in executor
- Do NOT store compositor output (silent video) as final_video_path — only store the post-render path
  </action>
  <verify>
1. Read the file and confirm all 6 pipeline stages are implemented with progress updates.
2. Confirm ElevenLabs path uses `generate_audio_with_timestamps()` (not `generate_audio()`).
3. Confirm Edge TTS path does NOT attempt to generate subtitles.
4. Confirm `editai_projects` and `editai_clips` inserts happen AFTER `_render_with_preset`.
5. Confirm error handling wraps the entire pipeline.
6. Run `python -c "from app.api.product_generate_routes import _generate_product_video_task"` — should import without errors.
  </verify>
  <done>
Background task implements the full pipeline: product fetch -> image resolve -> TTS voiceover (quick template or elaborate AI) -> SRT subtitles (ElevenLabs only) -> compositor (silent video) -> _render_with_preset (audio + encoding + filters + subtitles) -> library insert (project + clip rows). Job progress updates at each stage. Error handling catches failures and marks job as failed.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from app.api.product_generate_routes import router, ProductGenerateRequest, _generate_product_video_task"` imports successfully
2. `python -c "from app.main import app"` loads without errors (router registered)
3. Code review: `_generate_product_video_task` contains calls to `compose_product_video`, `get_tts_service` or `ElevenLabsTTSService`, `generate_srt_from_timestamps`, `_render_with_preset`, and Supabase `editai_clips` insert
4. Code review: Quick mode constructs text from template without AI call; elaborate mode uses `ScriptGenerator`
5. Code review: Progress string updates at stages 5%, 10%, 25%, 40%, 50%, 70%, 90%, 100%
</verification>

<success_criteria>
- POST /api/v1/products/{product_id}/generate endpoint exists, returns job_id
- Background task implements 6-stage pipeline with progress tracking
- Quick mode builds voiceover from template (TTS-01)
- Elaborate mode generates script via ScriptGenerator (TTS-02)
- TTS provider selection works for both edge and elevenlabs (TTS-03)
- Subtitles generated from ElevenLabs timestamps, skipped for Edge (TTS-04)
- Compositor + _render_with_preset used for final render (OUT-01, OUT-02, OUT-03)
- Video inserted into editai_projects + editai_clips (BATCH-05)
- Job status trackable via existing GET /api/v1/jobs/{job_id} (BATCH-01 backend)
</success_criteria>

<output>
After completion, create `.planning/phases/20-single-product-e2e/20-01-SUMMARY.md`
</output>
